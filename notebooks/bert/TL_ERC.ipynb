{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TL-ERC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-Te8AZ0M4QF",
        "outputId": "113ec8ea-ab79-4c98-9d67-45acac641088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'conv-emotion'...\n",
            "remote: Enumerating objects: 1388, done.\u001b[K\n",
            "remote: Counting objects: 100% (324/324), done.\u001b[K\n",
            "remote: Compressing objects: 100% (225/225), done.\u001b[K\n",
            "remote: Total 1388 (delta 156), reused 218 (delta 91), pack-reused 1064\u001b[K\n",
            "Receiving objects: 100% (1388/1388), 751.31 MiB | 13.87 MiB/s, done.\n",
            "Resolving deltas: 100% (700/700), done.\n",
            "Checking out files: 100% (435/435), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/declare-lab/conv-emotion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "_, videoSpeakers, videoLabels, _, _, _, videoSentence, trainVid, testVid = pickle.load(open(\"/content/drive/MyDrive/Depot_Mouhamed-20220228T100527Z-001/DialogueRNN/DialogueRNN/IEMOCAP_features/IEMOCAP_features_raw.pkl\", 'rb'), encoding='latin1')\n",
        "\n"
      ],
      "metadata": {
        "id": "-4bzeCp_SRBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(list(videoSentence.values())[0]))\n",
        "print(len(list(videoLabels.values())[0]))"
      ],
      "metadata": {
        "id": "_cZqx7a6TSm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61070cb-947b-4523-98f8-874a987cce62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n",
            "42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/conv-emotion/TL-ERC/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdXIks9UTauY",
        "outputId": "29462e0f-7ec4-4c2c-cbab-cd2b4bab9644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/conv-emotion/TL-ERC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py"
      ],
      "metadata": {
        "id": "awmEYIJR8v0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Depot_Mouhamed-20220228T100527Z-001/DialogueRNN/DialogueRNN/IEMOCAP_features/IEMOCAP_features_raw.pkl /content/conv-emotion/TL-ERC/datasets"
      ],
      "metadata": {
        "id": "rumJWWo_WkEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1OXtnyJ5nDMmK75L9kEQvKPIyO0xzyeVC\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HP6NaSq9npX",
        "outputId": "2a7dce5b-c478-4714-895b-524f7216c849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OXtnyJ5nDMmK75L9kEQvKPIyO0xzyeVC\n",
            "To: /content/conv-emotion/TL-ERC/cornell_weights.pkl\n",
            "100% 42.0M/42.0M [00:00<00:00, 78.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd bert_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7pJS4i__Dqz",
        "outputId": "d7c85d2e-ee1e-48c1-dbba-bb43a7bfa1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/conv-emotion/TL-ERC/bert_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --load_checkpoint=../generative_weights/cornell_weights.pkl --data=iemocap"
      ],
      "metadata": {
        "id": "I-vBVoT9_PCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1nufbrBJ-LtcROv1MviCHFI7tQE3JnqQR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-12u-WQzEMfK",
        "outputId": "c1dc0c5b-44c9-49f5-da53-b80ad6ae94e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nufbrBJ-LtcROv1MviCHFI7tQE3JnqQR\n",
            "To: /content/conv-emotion/TL-ERC/iemocap.zip\n",
            "100% 14.2M/14.2M [00:00<00:00, 44.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/conv-emotion/TL-ERC/iemocap.zip -d /content/conv-emotion/TL-ERC/datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMvW8y8AEhJP",
        "outputId": "76e89a71-cc10-43fc-e24f-1b50ef646c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/conv-emotion/TL-ERC/iemocap.zip\n",
            "   creating: /content/conv-emotion/TL-ERC/datasets/iemocap/\n",
            "   creating: /content/conv-emotion/TL-ERC/datasets/iemocap/valid/\n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/._.DS_Store  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/.DS_Store  \n",
            "   creating: /content/conv-emotion/TL-ERC/datasets/iemocap/test/\n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/IEMOCAP_features_raw.pkl  \n",
            "   creating: /content/conv-emotion/TL-ERC/datasets/iemocap/train/\n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/valid/video_id.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/valid/sentence_length.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/valid/sentences.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/valid/labels.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/valid/conversation_length.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/test/video_id.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/test/sentence_length.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/test/sentences.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/test/labels.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/test/conversation_length.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/train/video_id.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/train/sentence_length.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/train/sentences.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/train/labels.pkl  \n",
            "  inflating: /content/conv-emotion/TL-ERC/datasets/iemocap/train/conversation_length.pkl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from solver import *\n",
        "from data_loader import get_loader\n",
        "from configs import get_config\n",
        "from util import Vocab\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "\n",
        "config = get_config(parse = False, mode='train')\n",
        "val_config = get_config(parse = False, mode='valid')\n",
        "test_config = get_config(parse = False, mode='test')\n",
        "\n",
        "# _RUNS = config.runs\n",
        "\n",
        "# _best_test_loss, _best_test_f1_w, _best_test_f1_m, _best_epoch = [], [], [], []\n",
        "\n",
        "# for run in range(_RUNS):\n",
        "\n",
        "print(config)\n",
        "\n",
        "# No. of videos to consider\n",
        "training_data_len = int(config.training_percentage * \\\n",
        "    len(load_pickle(config.sentences_path)))\n",
        "\n",
        "\n",
        "train_data_loader = get_loader(\n",
        "    sentences=load_pickle(config.sentences_path)[:training_data_len],\n",
        "    labels=load_pickle(config.label_path)[:training_data_len],\n",
        "    conversation_length=load_pickle(config.conversation_length_path)[:training_data_len],\n",
        "    sentence_length=load_pickle(config.sentence_length_path)[:training_data_len],\n",
        "    batch_size=config.batch_size)\n",
        "\n",
        "eval_data_loader = get_loader(\n",
        "    sentences=load_pickle(val_config.sentences_path),\n",
        "    labels=load_pickle(val_config.label_path),\n",
        "    conversation_length=load_pickle(val_config.conversation_length_path),\n",
        "    sentence_length=load_pickle(val_config.sentence_length_path),\n",
        "    batch_size=val_config.eval_batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "test_data_loader = get_loader(\n",
        "    sentences=load_pickle(test_config.sentences_path),\n",
        "    labels=load_pickle(test_config.label_path),\n",
        "    conversation_length=load_pickle(test_config.conversation_length_path),\n",
        "    sentence_length=load_pickle(test_config.sentence_length_path),\n",
        "    batch_size=test_config.eval_batch_size,\n",
        "    shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for testing\n",
        "solver = Solver(config, train_data_loader,\n",
        "                eval_data_loader, test_data_loader, is_train=True)\n",
        "\n",
        "\n",
        "solver.build()\n",
        "\n",
        "    # best_test_loss, best_test_f1_w, best_epoch = solver.train()\n",
        "\n",
        "    # print(f\"Current RUN: {run+1}\")\n",
        "\n",
        "    # print(\"\\n\\nBest test loss\")\n",
        "    # print(best_test_loss)\n",
        "    # print(\"Best test f1 weighted\")\n",
        "    # print(best_test_f1_w)\n",
        "    # print(\"Best epoch\")\n",
        "    # print(best_epoch)\n",
        "\n",
        "    # _best_test_loss.append(best_test_loss)\n",
        "    # _best_test_f1_w.append(best_test_f1_w)\n",
        "    # _best_epoch.append(best_epoch)\n",
        "\n",
        "\n",
        "# Print final\n",
        "# print(f\"\\n\\nAverage across runs:\")\n",
        "\n",
        "# print(\"Best epoch\")\n",
        "# print(_best_epoch)\n",
        "\n",
        "# print(\"\\n\\nBest test loss\")\n",
        "# print(np.mean(np.array(_best_test_loss), axis=0))\n",
        "\n",
        "# print(\"Overall test f1 weighted\")\n",
        "# print(np.array(_best_test_f1_w))\n",
        "\n",
        "# print(\"Best test f1 weighted\")\n",
        "# print(np.mean(np.array(_best_test_f1_w), axis=0))"
      ],
      "metadata": {
        "id": "bQqwzQsE_jpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82cd750-43ed-418a-cea0-3138dd0cd01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iemocap\n",
            "iemocap\n",
            "iemocap\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/conv-emotion/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/conv-emotion/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/conv-emotion/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/conv-emotion/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/conv-emotion/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': None,\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/conv-emotion/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/conv-emotion/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/conv-emotion/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/conv-emotion/TL-ERC/datasets/iemocap/word_emb.pkl')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 425012.28B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build Graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 407873900/407873900 [00:27<00:00, 14715835.60B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Done! It took 3.8e+01 secs\n",
            "\n",
            "\n",
            "\n",
            "Average across runs:\n",
            "Best epoch\n",
            "[]\n",
            "\n",
            "\n",
            "Best test loss\n",
            "nan\n",
            "Overall test f1 weighted\n",
            "[]\n",
            "Best test f1 weighted\n",
            "nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_loss, w_valid_f1, valid_predictions = solver.evaluate(eval_data_loader, mode=\"valid\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_nNRV1eF84G",
        "outputId": "8d8db5ce-cb3d-4b29-8576-554e202c5910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [e[1] for e in eval_data_loader]\n",
        "Labels = [label for utt in labels for label in utt]\n",
        "Labels = [e for k in Labels for e in k]"
      ],
      "metadata": {
        "id": "uASIxVQNKTUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaQsTRQGKznX",
        "outputId": "9939715e-00c7-4cd3-de2d-7c49d97c2d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solver.print_metric(Labels, valid_predictions, \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTsuynDAI7ol",
        "outputId": "d7b4f0f1-27a5-40d0-9825-acaafc794a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0781    0.0391    0.0521       128\n",
            "           1     0.0909    0.0133    0.0233        75\n",
            "           2     0.0000    0.0000    0.0000       244\n",
            "           3     0.1720    0.9674    0.2920       184\n",
            "           4     0.0000    0.0000    0.0000       222\n",
            "           5     0.0000    0.0000    0.0000       258\n",
            "\n",
            "    accuracy                         0.1656      1111\n",
            "   macro avg     0.0568    0.1700    0.0612      1111\n",
            "weighted avg     0.0436    0.1656    0.0559      1111\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0559376253553394"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, w_test_f1, test_predictions = solver.evaluate(test_data_loader, mode=\"test\")"
      ],
      "metadata": {
        "id": "-1qGX-QqGT8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAiOsfDNA7g4",
        "outputId": "6f668ce7-5f8c-4d02-c14b-8c04f37b2dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.5)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3pCmId7BG8j",
        "outputId": "58cbe33c-45ec-483c-95fa-de1a98b3543d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.63.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.18-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 34.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.10.0.2)\n",
            "Collecting botocore<1.25.0,>=1.24.18\n",
            "  Downloading botocore-1.24.18-py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 49.8 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.18->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.18->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 51.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.18 botocore-1.24.18 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.2 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load or preprocess moseidata"
      ],
      "metadata": {
        "id": "UzjUiaUFWkgO"
      }
    }
  ]
}